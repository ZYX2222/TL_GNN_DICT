{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining VerticalGNN Models with hERG dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from model import VerticalGNN\n",
    "from config import NUM_FEATURES, NUM_TARGET, EDGE_DIM, DEVICE, SEED_NO, PATIENCE, EPOCHS, NUM_GRAPHS_PER_BATCH, N_SPLITS, best_params_vertical\n",
    "from engine import EnginehERG\n",
    "from utils import seed_everything, LoadhERGDataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training models with different epochs values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_trf_learning_model(train_loader, params, pretrained_model_path, epochs):\n",
    "\n",
    "    '''\n",
    "    Define function to pretrain model with solubililty dataset \n",
    "\n",
    "    Args:\n",
    "    train_loader: DataLoader class from pytorch geometric containing train dataset\n",
    "    params (dict): Dictionary containing hyperparameters\n",
    "    pretrained_model_path (str): path to save the pretrained model\n",
    "    epochs (int): Number of epochs to pretrain the model\n",
    "\n",
    "    Return:\n",
    "    loss: final train loss  \n",
    "    '''\n",
    "\n",
    "    model = VerticalGNN(num_features=NUM_FEATURES, num_targets=NUM_TARGET, num_gin_layers=params['num_gin_layers'], num_graph_trans_layers=params['num_graph_trans_layers'], hidden_size=params['hidden_size'], \n",
    "                        n_heads=params['n_heads'], dropout=params['dropout'], edge_dim=EDGE_DIM)         \n",
    "    model.to(DEVICE)\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr = params['learning_rate'])\n",
    "    eng = EnginehERG(model, optimizer, device=DEVICE)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = eng.train(train_loader)\n",
    "        print(f'Epoch: {epoch+1}/{epochs}, train loss : {train_loss}')\n",
    "        #print('Saving model...')\n",
    "        # Check and create directory before saving\n",
    "        if not os.path.exists(os.path.dirname(pretrained_model_path)):\n",
    "            os.makedirs(os.path.dirname(pretrained_model_path), exist_ok=True)\n",
    "        print('Saving model...')\n",
    "        torch.save(model.state_dict(), pretrained_model_path)\n",
    "\n",
    "        #torch.save(model.state_dict(), pretrained_model_path)\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10 #Setup on demand, such as 10-100\n",
    "params = best_params_vertical\n",
    "\n",
    "seed_everything(SEED_NO)\n",
    "train_dataset_mid = LoadhERGDataset(root='./data/graph_data/trf_learning_hERG/', raw_filename='hERG_data_for_pretrained_model.csv')\n",
    "train_loader_mid = DataLoader(train_dataset_mid, batch_size=64, shuffle=True)\n",
    "pretrained_model_path = f'./trf_learning_models/pretrained_models/vertical/pretrained_vertical_model_{epochs}_epoch.pt'\n",
    "\n",
    "train_loss = run_training_trf_learning_model(train_loader_mid, params, pretrained_model_path, epochs)\n",
    "print(f'train loss: {train_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
