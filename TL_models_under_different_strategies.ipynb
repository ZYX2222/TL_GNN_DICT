{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train transfer learning model for DICT prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required materials\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "from model import VerticalGNN\n",
    "from config import NUM_FEATURES, NUM_TARGET, EDGE_DIM, DEVICE, SEED_NO, PATIENCE, EPOCHS, NUM_GRAPHS_PER_BATCH, N_SPLITS, best_params_vertical\n",
    "from engine import EnginehERG, EngineDICT\n",
    "from utils import seed_everything, LoadDICTDataset, LoadhERGDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a train and test function that can aid us in transfer learning. Then, we evaluate how different factors can affect the results of transfer learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(method_tf, train_loader, valid_loader, params,es_trigger, path_to_pretrained_model, path_to_save_trained_model):\n",
    "    \n",
    "    '''\n",
    "    Define a function to wrap training\n",
    "\n",
    "    Args:\n",
    "    method_tf (str): freeze --> freeze parameters of feature extraction block, fine_tune_2x -> fine tune at 2x slower learning rate, \n",
    "                    fine_tune_5x -> fine tune at 5x slower learning rate\n",
    "    train_loader: DataLoader class from pytorch geometric containing train data\n",
    "    valid_loader: DataLoader class from pytorch geometric containing validation data\n",
    "    params (dict): dictionary containing the hyperparameters\n",
    "    es_trigger (int): a number to force train model before triggering early stopping mechanism \n",
    "    path_to_pretrained_model (str): path to load the pretrained models\n",
    "    path_to_save_trained_model: path to save the trained models\n",
    "\n",
    "    Return:\n",
    "    best loss: return best validation loss\n",
    "    '''\n",
    "    \n",
    "    model = VerticalGNN(\n",
    "            num_features=NUM_FEATURES,\n",
    "            num_targets=NUM_TARGET,\n",
    "            num_gin_layers=params[\"num_gin_layers\"],\n",
    "            num_graph_trans_layers=params[\"num_graph_trans_layers\"],\n",
    "            hidden_size=params[\"hidden_size\"],\n",
    "            n_heads=params[\"n_heads\"],\n",
    "            dropout=params[\"dropout\"],\n",
    "            edge_dim=EDGE_DIM,\n",
    "        )\n",
    "\n",
    "    model.load_state_dict(torch.load(path_to_pretrained_model))  \n",
    "    model.to(DEVICE)\n",
    "    if method_tf == 'freeze':\n",
    "        for param in model.gin_model.parameters():\n",
    "            param.requires_grad=False\n",
    "        for param in model.graph_trans_model.parameters():\n",
    "            param.requires_grad=False\n",
    "\n",
    "        optimizer=torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = params['learning_rate'])\n",
    "    \n",
    "    elif method_tf == 'fine_tune_5x':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']/5},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']/5},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "\n",
    "    elif method_tf == 'fine_tune_10x':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']/10},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']/10},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "\n",
    "    elif method_tf == 'fine_tune':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "\n",
    "    elif method_tf == 'fine_tune_2x':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']/2},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']/2},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "        \n",
    "    eng = EngineDICT(model, optimizer, device=DEVICE)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    early_stopping_iter = PATIENCE\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = eng.train(train_loader)\n",
    "        valid_loss_tuple = eng.validate(valid_loader)\n",
    "        valid_loss = valid_loss_tuple[0]\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1}/{EPOCHS}, train loss : {train_loss}, validation loss : {valid_loss}\"\n",
    "        )\n",
    "        if epoch+1>es_trigger:\n",
    "            if valid_loss < best_loss:\n",
    "                best_loss = valid_loss\n",
    "                early_stopping_counter = 0  # reset counter\n",
    "                \n",
    "                model_save_directory = os.path.dirname(path_to_save_trained_model)\n",
    "                if not os.path.exists(model_save_directory):\n",
    "                    os.makedirs(model_save_directory, exist_ok=True)\n",
    "\n",
    "                print(\"Saving model...\")\n",
    "                torch.save(model.state_dict(), path_to_save_trained_model)\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "\n",
    "            if early_stopping_counter > early_stopping_iter:\n",
    "                print(\"Early stopping...\")\n",
    "                break\n",
    "            print(f\"Early stop counter: {early_stopping_counter}\")\n",
    "\n",
    "    return best_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(method_tf, valid_loader, params, path_to_trained_model):\n",
    "    model = VerticalGNN(\n",
    "            num_features=NUM_FEATURES,\n",
    "            num_targets=NUM_TARGET,\n",
    "            num_gin_layers=params[\"num_gin_layers\"],\n",
    "            num_graph_trans_layers=params[\"num_graph_trans_layers\"],\n",
    "            hidden_size=params[\"hidden_size\"],\n",
    "            n_heads=params[\"n_heads\"],\n",
    "            dropout=params[\"dropout\"],\n",
    "            edge_dim=EDGE_DIM,\n",
    "        )\n",
    "\n",
    "    model.load_state_dict(torch.load(path_to_trained_model))  \n",
    "    model.to(DEVICE)\n",
    "    if method_tf == 'freeze':\n",
    "        for param in model.gin_model.parameters():\n",
    "            param.requires_grad=False\n",
    "        for param in model.graph_trans_model.parameters():\n",
    "            param.requires_grad=False\n",
    "\n",
    "        optimizer=torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = params['learning_rate'])        \n",
    "    \n",
    "    elif method_tf == 'fine_tune_5x':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']/5},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']/5},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "        \n",
    "\n",
    "    elif method_tf == 'fine_tune_10x':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']/10},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']/10},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "\n",
    "    elif method_tf == 'fine_tune':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "\n",
    "    elif method_tf == 'fine_tune_2x':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']/2},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']/2},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "        \n",
    "\n",
    "    eng = EngineDICT(model, optimizer, device=DEVICE)\n",
    "    bce, acc, f1, roc_auc = eng.validate(valid_loader)\n",
    "    print(f\"bce:{bce}, acc :{acc}, f1: {f1}, roc_auc: {roc_auc}\")\n",
    "    return bce, acc, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_testing(method_tf, test_loader, params, path_to_trained_model):\n",
    "    \n",
    "    model = VerticalGNN(\n",
    "            num_features=NUM_FEATURES,\n",
    "            num_targets=NUM_TARGET,\n",
    "            num_gin_layers=params[\"num_gin_layers\"],\n",
    "            num_graph_trans_layers=params[\"num_graph_trans_layers\"],\n",
    "            hidden_size=params[\"hidden_size\"],\n",
    "            n_heads=params[\"n_heads\"],\n",
    "            dropout=params[\"dropout\"],\n",
    "            edge_dim=EDGE_DIM,\n",
    "        )\n",
    "\n",
    "    model.load_state_dict(torch.load(path_to_trained_model))  \n",
    "    model.to(DEVICE)\n",
    "    if method_tf == 'freeze':\n",
    "        for param in model.gin_model.parameters():\n",
    "            param.requires_grad=False\n",
    "        for param in model.graph_trans_model.parameters():\n",
    "            param.requires_grad=False\n",
    "\n",
    "        optimizer=torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = params['learning_rate'])        \n",
    "    \n",
    "    elif method_tf == 'fine_tune_5x':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']/5},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']/5},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "        \n",
    "\n",
    "    elif method_tf == 'fine_tune_10x':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']/10},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']/10},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "        \n",
    "    elif method_tf == 'fine_tune':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "\n",
    "    elif method_tf == 'fine_tune_2x':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']/2},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']/2},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "        \n",
    "\n",
    "    eng = EngineDICT(model, optimizer, device=DEVICE)\n",
    "    bce, acc, f1, roc_auc = eng.test(test_loader)\n",
    "    print(f\"bce:{bce}, acc :{acc}, f1: {f1}, roc_auc: {roc_auc}\")\n",
    "    return bce, acc, f1, roc_auc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of number of pre-training epochs to transfer learning model performance\n",
    "1. Weights are first frozen for the feature extraction block. \n",
    "2. Models are allowed to train and only weights for the classifier block is allowed to be updated. \n",
    "3. To evaluate how number of pre-training epochs affect the transfer learning prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_root_path = './data/graph_data/data_DICT_train/'\n",
    "train_data_raw_filename = 'data_DICT_train.csv'\n",
    "test_data_root_path = './data/graph_data/data_DICT_test'\n",
    "test_data_raw_filename = 'data_DICT_test.csv'\n",
    "n_repetitions = 1\n",
    "method_tf = 'freeze'\n",
    "params = best_params_vertical\n",
    "es_trigger = 0\n",
    "path_to_pretrained_model = './trf_learning_models/pretrained_models/vertical/'\n",
    "path_to_save_trained_model = './trf_learning_models/trained_models/vertical/pretrained_40/'    # Setup on demand\n",
    "path_to_trained_model = './trf_learning_models/trained_models/vertical/pretrained_40/'   # Setup on demand\n",
    "\n",
    "val_bce_list = []\n",
    "val_acc_list = []\n",
    "val_f1_list = []\n",
    "val_roc_auc_list = []\n",
    "\n",
    "bce_list = []\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "dataset_for_cv = LoadDICTDataset(train_data_root_path, train_data_raw_filename)\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "for repeat in range(n_repetitions):\n",
    "    repeat_val_bce_list = []\n",
    "    repeat_val_acc_list = []\n",
    "    repeat_val_f1_list = []\n",
    "    repeat_val_roc_auc_list = []\n",
    "    \n",
    "    repeat_bce_list = []\n",
    "    repeat_acc_list = []\n",
    "    repeat_f1_list = []\n",
    "    repeat_roc_auc_list = []\n",
    "    \n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        seed_everything(SEED_NO)\n",
    "        train_dataset = []\n",
    "        valid_dataset = []\n",
    "        \n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_DICT_train/processed/molecule_{t_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_DICT_train/processed/molecule_{v_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        test_dataset = LoadDICTDataset(test_data_root_path, test_data_raw_filename)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        print(f'Rep no {repeat}, Fold no {fold_no}')\n",
    "        \n",
    "        run_training(method_tf, train_loader, valid_loader, params, es_trigger, os.path.join(path_to_pretrained_model, f'pretrained_vertical_model_40_epoch.pt'),\n",
    "            os.path.join(\n",
    "                path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        val_bce, val_acc, val_f1, val_roc_auc = run_validation(method_tf, valid_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "        bce, acc, f1, roc_auc = run_testing(method_tf, test_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "\n",
    "        repeat_val_bce_list.append(val_bce)\n",
    "        repeat_val_acc_list.append(val_acc)\n",
    "        repeat_val_f1_list.append(val_f1)\n",
    "        repeat_val_roc_auc_list.append(val_roc_auc)\n",
    "        \n",
    "        repeat_bce_list.append(bce)\n",
    "        repeat_acc_list.append(acc)\n",
    "        repeat_f1_list.append(f1)\n",
    "        repeat_roc_auc_list.append(roc_auc)\n",
    "        \n",
    "        val_bce_list.append(val_bce)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_f1_list.append(val_f1)\n",
    "        val_roc_auc_list.append(val_roc_auc)\n",
    "        \n",
    "        bce_list.append(bce)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "    # Output statistics for validation and CV results for the repeat\n",
    "    print(f'Statistics for repeat {repeat}:')\n",
    "    print(f'Validation - BCE: {np.mean(repeat_val_bce_list):.3f}±{np.std(repeat_val_bce_list):.3f}')\n",
    "    print(f'Validation - ACC: {np.mean(repeat_val_acc_list):.3f}±{np.std(repeat_val_acc_list):.3f}')\n",
    "    print(f'Validation - F1: {np.mean(repeat_val_f1_list):.3f}±{np.std(repeat_val_f1_list):.3f}')\n",
    "    print(f'Validation - ROC_AUC: {np.mean(repeat_val_roc_auc_list):.3f}±{np.std(repeat_val_roc_auc_list):.3f}')\n",
    "\n",
    "    print(f'test - BCE: {np.mean(repeat_bce_list):.3f}±{np.std(repeat_bce_list):.3f}')\n",
    "    print(f'test - ACC: {np.mean(repeat_acc_list):.3f}±{np.std(repeat_acc_list):.3f}')\n",
    "    print(f'test - F1: {np.mean(repeat_f1_list):.3f}±{np.std(repeat_f1_list):.3f}')\n",
    "    print(f'test - ROC_AUC: {np.mean(repeat_roc_auc_list):.3f}±{np.std(repeat_roc_auc_list):.3f}')\n",
    "\n",
    "val_bce_arr = np.array(val_bce_list)\n",
    "val_mean_bce = np.mean(val_bce_arr)\n",
    "val_sd_bce = np.std(val_bce_arr)\n",
    "print(f'validation bce:{val_mean_bce:.3f}±{val_sd_bce:.3f}')\n",
    "\n",
    "val_acc_arr = np.array(val_acc_list)\n",
    "val_acc_mean= np.mean(val_acc_arr)\n",
    "val_acc_sd = np.std(val_acc_arr)\n",
    "print(f'validation acc:{val_acc_mean:.3f}±{val_acc_sd:.3f}')\n",
    "\n",
    "val_f1_arr = np.array(val_f1_list)\n",
    "val_f1_mean= np.mean(val_f1_arr)\n",
    "val_f1_sd = np.std(val_f1_arr)\n",
    "print(f'validation f1: {val_f1_mean:.3f}±{val_f1_sd:.3f}')\n",
    "\n",
    "val_roc_auc_arr = np.array(val_roc_auc_list)\n",
    "val_roc_auc_mean= np.mean(val_roc_auc_arr)\n",
    "val_roc_auc_sd = np.std(val_roc_auc_arr)\n",
    "print(f'validation roc_auc: {val_roc_auc_mean:.3f}±{val_roc_auc_sd:.3f}')\n",
    "\n",
    "bce_arr = np.array(bce_list)\n",
    "mean_bce = np.mean(bce_arr)\n",
    "sd_bce = np.std(bce_arr)\n",
    "print(f'bce:{mean_bce:.3f}±{sd_bce:.3f}')\n",
    "\n",
    "acc_arr = np.array(acc_list)\n",
    "acc_mean= np.mean(acc_arr)\n",
    "acc_sd = np.std(acc_arr)\n",
    "print(f'acc:{acc_mean:.3f}±{acc_sd:.3f}')\n",
    "\n",
    "f1_arr = np.array(f1_list)\n",
    "f1_mean= np.mean(f1_arr)\n",
    "f1_sd = np.std(f1_arr)\n",
    "print(f'f1: {f1_mean:.3f}±{f1_sd:.3f}')\n",
    "\n",
    "roc_auc_arr = np.array(roc_auc_list)\n",
    "roc_auc_mean= np.mean(roc_auc_arr)\n",
    "roc_auc_sd = np.std(roc_auc_arr)\n",
    "print(f'roc_auc: {roc_auc_mean:.3f}±{roc_auc_sd:.3f}')\n",
    "\n",
    "print(\"Training Completed!\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of reducing learning rates to transfer learning prediction performance\n",
    "Fine_tune = No change in learning rate  \n",
    "Fine_tune_2x = 2-fold reduced in learning rate  \n",
    "Fine_tune_5x = 5-fold reduced in learning rate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_root_path = './data/graph_data/data_DICT_train/'\n",
    "train_data_raw_filename = 'data_DICT_train.csv'\n",
    "test_data_root_path = './data/graph_data/data_DICT_test'\n",
    "test_data_raw_filename = 'data_DICT_test.csv'\n",
    "n_repetitions = 1\n",
    "method_tf = 'fine_tune' # Setup on demand\n",
    "params = best_params_vertical\n",
    "es_trigger = 0\n",
    "path_to_pretrained_model = './trf_learning_models/pretrained_models/vertical/'\n",
    "path_to_save_trained_model = './trf_learning_models/trained_models/vertical/pretrained_40/'\n",
    "path_to_trained_model = './trf_learning_models/trained_models/vertical/pretrained_40/'\n",
    "\n",
    "val_bce_list = []\n",
    "val_acc_list = []\n",
    "val_f1_list = []\n",
    "val_roc_auc_list = []\n",
    "\n",
    "bce_list = []\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "dataset_for_cv = LoadDICTDataset(train_data_root_path, train_data_raw_filename)\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "for repeat in range(n_repetitions):\n",
    "    repeat_val_bce_list = []\n",
    "    repeat_val_acc_list = []\n",
    "    repeat_val_f1_list = []\n",
    "    repeat_val_roc_auc_list = []\n",
    "    \n",
    "    repeat_bce_list = []\n",
    "    repeat_acc_list = []\n",
    "    repeat_f1_list = []\n",
    "    repeat_roc_auc_list = []\n",
    "    \n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        seed_everything(SEED_NO)\n",
    "        train_dataset = []\n",
    "        valid_dataset = []\n",
    "        \n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_DICT_train/processed/molecule_{t_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_DICT_train/processed/molecule_{v_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        test_dataset = LoadDICTDataset(test_data_root_path, test_data_raw_filename)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        print(f'Rep no {repeat}, Fold no {fold_no}')\n",
    "        \n",
    "        run_training(method_tf, train_loader, valid_loader, params, es_trigger, os.path.join(path_to_pretrained_model, f'pretrained_vertical_model_40_epoch.pt'),\n",
    "            os.path.join(\n",
    "                path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        val_bce, val_acc, val_f1, val_roc_auc = run_validation(method_tf, valid_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "        bce, acc, f1, roc_auc = run_testing(method_tf, test_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "\n",
    "        repeat_val_bce_list.append(val_bce)\n",
    "        repeat_val_acc_list.append(val_acc)\n",
    "        repeat_val_f1_list.append(val_f1)\n",
    "        repeat_val_roc_auc_list.append(val_roc_auc)\n",
    "        \n",
    "        repeat_bce_list.append(bce)\n",
    "        repeat_acc_list.append(acc)\n",
    "        repeat_f1_list.append(f1)\n",
    "        repeat_roc_auc_list.append(roc_auc)\n",
    "        \n",
    "        val_bce_list.append(val_bce)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_f1_list.append(val_f1)\n",
    "        val_roc_auc_list.append(val_roc_auc)\n",
    "        \n",
    "        bce_list.append(bce)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "    # Output statistics for validation and CV results for the repeat\n",
    "    print(f'Statistics for repeat {repeat}:')\n",
    "    print(f'Validation - BCE: {np.mean(repeat_val_bce_list):.3f}±{np.std(repeat_val_bce_list):.3f}')\n",
    "    print(f'Validation - ACC: {np.mean(repeat_val_acc_list):.3f}±{np.std(repeat_val_acc_list):.3f}')\n",
    "    print(f'Validation - F1: {np.mean(repeat_val_f1_list):.3f}±{np.std(repeat_val_f1_list):.3f}')\n",
    "    print(f'Validation - ROC_AUC: {np.mean(repeat_val_roc_auc_list):.3f}±{np.std(repeat_val_roc_auc_list):.3f}')\n",
    "\n",
    "    print(f'test - BCE: {np.mean(repeat_bce_list):.3f}±{np.std(repeat_bce_list):.3f}')\n",
    "    print(f'test - ACC: {np.mean(repeat_acc_list):.3f}±{np.std(repeat_acc_list):.3f}')\n",
    "    print(f'test - F1: {np.mean(repeat_f1_list):.3f}±{np.std(repeat_f1_list):.3f}')\n",
    "    print(f'test - ROC_AUC: {np.mean(repeat_roc_auc_list):.3f}±{np.std(repeat_roc_auc_list):.3f}')\n",
    "\n",
    "val_bce_arr = np.array(val_bce_list)\n",
    "val_mean_bce = np.mean(val_bce_arr)\n",
    "val_sd_bce = np.std(val_bce_arr)\n",
    "print(f'validation bce:{val_mean_bce:.3f}±{val_sd_bce:.3f}')\n",
    "\n",
    "val_acc_arr = np.array(val_acc_list)\n",
    "val_acc_mean= np.mean(val_acc_arr)\n",
    "val_acc_sd = np.std(val_acc_arr)\n",
    "print(f'validation acc:{val_acc_mean:.3f}±{val_acc_sd:.3f}')\n",
    "\n",
    "val_f1_arr = np.array(val_f1_list)\n",
    "val_f1_mean= np.mean(val_f1_arr)\n",
    "val_f1_sd = np.std(val_f1_arr)\n",
    "print(f'validation f1: {val_f1_mean:.3f}±{val_f1_sd:.3f}')\n",
    "\n",
    "val_roc_auc_arr = np.array(val_roc_auc_list)\n",
    "val_roc_auc_mean= np.mean(val_roc_auc_arr)\n",
    "val_roc_auc_sd = np.std(val_roc_auc_arr)\n",
    "print(f'validation roc_auc: {val_roc_auc_mean:.3f}±{val_roc_auc_sd:.3f}')\n",
    "\n",
    "bce_arr = np.array(bce_list)\n",
    "mean_bce = np.mean(bce_arr)\n",
    "sd_bce = np.std(bce_arr)\n",
    "print(f'bce:{mean_bce:.3f}±{sd_bce:.3f}')\n",
    "\n",
    "acc_arr = np.array(acc_list)\n",
    "acc_mean= np.mean(acc_arr)\n",
    "acc_sd = np.std(acc_arr)\n",
    "print(f'acc:{acc_mean:.3f}±{acc_sd:.3f}')\n",
    "\n",
    "f1_arr = np.array(f1_list)\n",
    "f1_mean= np.mean(f1_arr)\n",
    "f1_sd = np.std(f1_arr)\n",
    "print(f'f1: {f1_mean:.3f}±{f1_sd:.3f}')\n",
    "\n",
    "roc_auc_arr = np.array(roc_auc_list)\n",
    "roc_auc_mean= np.mean(roc_auc_arr)\n",
    "roc_auc_sd = np.std(roc_auc_arr)\n",
    "print(f'roc_auc: {roc_auc_mean:.3f}±{roc_auc_sd:.3f}')\n",
    "\n",
    "print(\"Training Completed!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of epoches before triggering the early stopping mechanism  to transfer learning prediction performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_root_path = './data/graph_data/data_DICT_train/'\n",
    "train_data_raw_filename = 'data_DICT_train.csv'\n",
    "test_data_root_path = './data/graph_data/data_DICT_test'\n",
    "test_data_raw_filename = 'data_DICT_test.csv'\n",
    "n_repetitions = 1\n",
    "method_tf = 'fine_tune_2x'\n",
    "params = best_params_vertical\n",
    "es_trigger = 0  # Setup on demand\n",
    "path_to_pretrained_model = './trf_learning_models/pretrained_models/vertical/'\n",
    "path_to_save_trained_model = './trf_learning_models/trained_models/vertical/pretrained_40/'\n",
    "path_to_trained_model = './trf_learning_models/trained_models/vertical/pretrained_40/'\n",
    "\n",
    "val_bce_list = []\n",
    "val_acc_list = []\n",
    "val_f1_list = []\n",
    "val_roc_auc_list = []\n",
    "\n",
    "bce_list = []\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "dataset_for_cv = LoadDICTDataset(train_data_root_path, train_data_raw_filename)\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "for repeat in range(n_repetitions):\n",
    "    repeat_val_bce_list = []\n",
    "    repeat_val_acc_list = []\n",
    "    repeat_val_f1_list = []\n",
    "    repeat_val_roc_auc_list = []\n",
    "    \n",
    "    repeat_bce_list = []\n",
    "    repeat_acc_list = []\n",
    "    repeat_f1_list = []\n",
    "    repeat_roc_auc_list = []\n",
    "    \n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        seed_everything(SEED_NO)\n",
    "        train_dataset = []\n",
    "        valid_dataset = []\n",
    "        \n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_DICT_train/processed/molecule_{t_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_DICT_train/processed/molecule_{v_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        test_dataset = LoadDICTDataset(test_data_root_path, test_data_raw_filename)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        print(f'Rep no {repeat}, Fold no {fold_no}')\n",
    "        \n",
    "        run_training(method_tf, train_loader, valid_loader, params, es_trigger, os.path.join(path_to_pretrained_model, f'pretrained_vertical_model_40_epoch.pt'),\n",
    "            os.path.join(\n",
    "                path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        val_bce, val_acc, val_f1, val_roc_auc = run_validation(method_tf, valid_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "        bce, acc, f1, roc_auc = run_testing(method_tf, test_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "\n",
    "        repeat_val_bce_list.append(val_bce)\n",
    "        repeat_val_acc_list.append(val_acc)\n",
    "        repeat_val_f1_list.append(val_f1)\n",
    "        repeat_val_roc_auc_list.append(val_roc_auc)\n",
    "        \n",
    "        repeat_bce_list.append(bce)\n",
    "        repeat_acc_list.append(acc)\n",
    "        repeat_f1_list.append(f1)\n",
    "        repeat_roc_auc_list.append(roc_auc)\n",
    "        \n",
    "        val_bce_list.append(val_bce)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_f1_list.append(val_f1)\n",
    "        val_roc_auc_list.append(val_roc_auc)\n",
    "        \n",
    "        bce_list.append(bce)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "    # Output statistics for validation and CV results for the repeat\n",
    "    print(f'Statistics for repeat {repeat}:')\n",
    "    print(f'Validation - BCE: {np.mean(repeat_val_bce_list):.3f}±{np.std(repeat_val_bce_list):.3f}')\n",
    "    print(f'Validation - ACC: {np.mean(repeat_val_acc_list):.3f}±{np.std(repeat_val_acc_list):.3f}')\n",
    "    print(f'Validation - F1: {np.mean(repeat_val_f1_list):.3f}±{np.std(repeat_val_f1_list):.3f}')\n",
    "    print(f'Validation - ROC_AUC: {np.mean(repeat_val_roc_auc_list):.3f}±{np.std(repeat_val_roc_auc_list):.3f}')\n",
    "\n",
    "    print(f'test - BCE: {np.mean(repeat_bce_list):.3f}±{np.std(repeat_bce_list):.3f}')\n",
    "    print(f'test - ACC: {np.mean(repeat_acc_list):.3f}±{np.std(repeat_acc_list):.3f}')\n",
    "    print(f'test - F1: {np.mean(repeat_f1_list):.3f}±{np.std(repeat_f1_list):.3f}')\n",
    "    print(f'test - ROC_AUC: {np.mean(repeat_roc_auc_list):.3f}±{np.std(repeat_roc_auc_list):.3f}')\n",
    "\n",
    "val_bce_arr = np.array(val_bce_list)\n",
    "val_mean_bce = np.mean(val_bce_arr)\n",
    "val_sd_bce = np.std(val_bce_arr)\n",
    "print(f'validation bce:{val_mean_bce:.3f}±{val_sd_bce:.3f}')\n",
    "\n",
    "val_acc_arr = np.array(val_acc_list)\n",
    "val_acc_mean= np.mean(val_acc_arr)\n",
    "val_acc_sd = np.std(val_acc_arr)\n",
    "print(f'validation acc:{val_acc_mean:.3f}±{val_acc_sd:.3f}')\n",
    "\n",
    "val_f1_arr = np.array(val_f1_list)\n",
    "val_f1_mean= np.mean(val_f1_arr)\n",
    "val_f1_sd = np.std(val_f1_arr)\n",
    "print(f'validation f1: {val_f1_mean:.3f}±{val_f1_sd:.3f}')\n",
    "\n",
    "val_roc_auc_arr = np.array(val_roc_auc_list)\n",
    "val_roc_auc_mean= np.mean(val_roc_auc_arr)\n",
    "val_roc_auc_sd = np.std(val_roc_auc_arr)\n",
    "print(f'validation roc_auc: {val_roc_auc_mean:.3f}±{val_roc_auc_sd:.3f}')\n",
    "\n",
    "bce_arr = np.array(bce_list)\n",
    "mean_bce = np.mean(bce_arr)\n",
    "sd_bce = np.std(bce_arr)\n",
    "print(f'bce:{mean_bce:.3f}±{sd_bce:.3f}')\n",
    "\n",
    "acc_arr = np.array(acc_list)\n",
    "acc_mean= np.mean(acc_arr)\n",
    "acc_sd = np.std(acc_arr)\n",
    "print(f'acc:{acc_mean:.3f}±{acc_sd:.3f}')\n",
    "\n",
    "f1_arr = np.array(f1_list)\n",
    "f1_mean= np.mean(f1_arr)\n",
    "f1_sd = np.std(f1_arr)\n",
    "print(f'f1: {f1_mean:.3f}±{f1_sd:.3f}')\n",
    "\n",
    "roc_auc_arr = np.array(roc_auc_list)\n",
    "roc_auc_mean= np.mean(roc_auc_arr)\n",
    "roc_auc_sd = np.std(roc_auc_arr)\n",
    "print(f'roc_auc: {roc_auc_mean:.3f}±{roc_auc_sd:.3f}')\n",
    "\n",
    "print(\"Training Completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
