{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertical GNN Model for DICT Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import seed_everything, LoadDICTDataset\n",
    "from config import SEED_NO, NUM_FEATURES, NUM_GRAPHS_PER_BATCH, NUM_TARGET, EDGE_DIM, DEVICE, PATIENCE, EPOCHS, N_SPLITS, params_vertical_gnn\n",
    "from engine import EngineDICT\n",
    "from model import VerticalGNN\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "from torch_geometric.loader import DataLoader\n",
    "import os "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tuning(train_loader, valid_loader, params):\n",
    "    model = VerticalGNN(num_features=NUM_FEATURES, num_targets=NUM_TARGET, num_gin_layers=params['num_gin_layers'], num_graph_trans_layers=params['num_graph_trans_layers'], \n",
    "                            hidden_size=params['hidden_size'], n_heads=params['n_heads'], dropout=params['dropout'], edge_dim=EDGE_DIM)\n",
    "    model.to(DEVICE)\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr = params['learning_rate'])\n",
    "    eng = EngineDICT(model, optimizer, device=DEVICE)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    early_stopping_iter = PATIENCE\n",
    "    early_stopping_counter = 0 \n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = eng.train(train_loader)\n",
    "        valid_loss_tuple = eng.validate(valid_loader)\n",
    "        valid_loss = valid_loss_tuple[0]\n",
    "        print(f'Epoch: {epoch+1}/{EPOCHS}, train loss : {train_loss}, validation loss : {valid_loss}')\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss \n",
    "            early_stopping_counter=0\n",
    "\n",
    "        else:\n",
    "            early_stopping_counter +=1\n",
    "\n",
    "        if early_stopping_counter > early_stopping_iter:\n",
    "            print('Early stopping...')\n",
    "            break\n",
    "        print(f'Early stop counter: {early_stopping_counter}')\n",
    "    \n",
    "    return best_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'num_gin_layers' : trial.suggest_categorical('num_gin_layers', [1, 2, 3]),\n",
    "        'num_graph_trans_layers' : trial.suggest_categorical('num_graph_trans_layers', [1, 2, 3]),\n",
    "        'hidden_size' : trial.suggest_categorical('hidden_size', [64, 128, 256]),\n",
    "        'n_heads' : trial.suggest_categorical('n_heads', [1, 2, 3]),\n",
    "        'dropout': trial.suggest_categorical('dropout', [0.1, 0.2, 0.3, 0.4]),\n",
    "        'learning_rate' : trial.suggest_categorical('learning_rate', [1e-3, 3e-3, 5e-3, 7e-3, 9e-3])\n",
    "    }\n",
    "    \n",
    "    \n",
    "    #load dataset \n",
    "    dataset_for_cv = LoadDICTDataset(root='./data/graph_data/data_DICT_train/', raw_filename='data_DICT_train.csv')\n",
    "    kf = KFold(n_splits=N_SPLITS)\n",
    "    fold_loss = 0\n",
    "\n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        print(f'Fold {fold_no}')\n",
    "        train_dataset= []\n",
    "        valid_dataset = []\n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(torch.load(f'./data/graph_data/data_DICT_train/processed/molecule_{t_idx}.pt'))\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(torch.load(f'./data/graph_data/data_DICT_train/processed/molecule_{v_idx}.pt'))\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False)\n",
    "\n",
    "        loss = run_tuning(train_loader, valid_loader, params)\n",
    "        fold_loss += loss\n",
    "\n",
    "    return fold_loss/10\n",
    "if __name__ == '__main__':\n",
    "    study = optuna.create_study(direction = 'minimize')\n",
    "    study.optimize(objective, n_trials=30)\n",
    "    print(f'best trial:')\n",
    "    trial_ = study.best_trial\n",
    "    print(trial_.values)\n",
    "    print(f'Best parameters: {trial_.params}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/validate/test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_loader, valid_loader, params, trained_model_path):\n",
    "    model = VerticalGNN(num_features=NUM_FEATURES, num_targets=NUM_TARGET, num_gin_layers=params['num_gin_layers'], num_graph_trans_layers=params['num_graph_trans_layers'], \n",
    "                            hidden_size=params['hidden_size'], n_heads=params['n_heads'], dropout=params['dropout'], edge_dim=EDGE_DIM)\n",
    "    model.to(DEVICE)\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr = params['learning_rate'])\n",
    "    eng = EngineDICT(model, optimizer, device=DEVICE)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    early_stopping_iter = PATIENCE\n",
    "    early_stopping_counter = 0 \n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = eng.train(train_loader)\n",
    "        valid_loss_tuple = eng.validate(valid_loader)\n",
    "        valid_loss = valid_loss_tuple[0]\n",
    "        print(f'Epoch: {epoch+1}/{EPOCHS}, train loss : {train_loss}, validation loss : {valid_loss}')\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss \n",
    "            early_stopping_counter=0 #reset counter\n",
    "            print('Saving model...')\n",
    "            \n",
    "            os.makedirs(os.path.dirname(trained_model_path), exist_ok=True)\n",
    "\n",
    "            torch.save(model.state_dict(), trained_model_path)\n",
    "        else:\n",
    "            early_stopping_counter +=1\n",
    "\n",
    "        if early_stopping_counter > early_stopping_iter:\n",
    "            print('Early stopping...')\n",
    "            break\n",
    "        print(f'Early stop counter: {early_stopping_counter}')\n",
    "    \n",
    "    return best_loss\n",
    "\n",
    "def run_validation(valid_loader, params, trained_model_path):\n",
    "    model = VerticalGNN(num_features=NUM_FEATURES, num_targets=NUM_TARGET, num_gin_layers=params['num_gin_layers'], num_graph_trans_layers=params['num_graph_trans_layers'], \n",
    "                            hidden_size=params['hidden_size'], n_heads=params['n_heads'], dropout=params['dropout'], edge_dim=EDGE_DIM)\n",
    "    model.load_state_dict(torch.load(trained_model_path))\n",
    "    model.to(DEVICE)\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr = params['learning_rate'])\n",
    "    eng = EngineDICT(model, optimizer, device=DEVICE)\n",
    "    bce, acc, f1, roc_auc = eng.validate(valid_loader)\n",
    "    print(f\"bce:{bce}, acc :{acc}, f1: {f1}, roc_auc: {roc_auc}\")\n",
    "    return bce, acc, f1, roc_auc\n",
    "\n",
    "\n",
    "def run_testing(test_loader, params, trained_model_path):\n",
    "    model = VerticalGNN(num_features=NUM_FEATURES, num_targets=NUM_TARGET, num_gin_layers=params['num_gin_layers'], num_graph_trans_layers=params['num_graph_trans_layers'], \n",
    "                            hidden_size=params['hidden_size'], n_heads=params['n_heads'], dropout=params['dropout'], edge_dim=EDGE_DIM)\n",
    "    model.load_state_dict(torch.load(trained_model_path))\n",
    "    model.to(DEVICE)\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr = params['learning_rate'])\n",
    "    eng = EngineDICT(model, optimizer, device=DEVICE)\n",
    "\n",
    "    #print('Begin testing...')\n",
    "    bce, acc, f1, roc_auc, all_labels, all_predictions = eng.test(test_loader)\n",
    "    #print('Test completed!')\n",
    "    print(f'bce:{bce}, acc:{acc}, f1: {f1}, roc_auc: {roc_auc}')\n",
    "    \n",
    "    # Save the predicted labels to a CSV file\n",
    "    predictions_file = f\"{trained_model_path[:-3]}_pre_test.csv\"  # Remove .pt and add suffix\n",
    "    save_predictions_to_csv(all_labels, all_predictions, predictions_file)\n",
    "    \n",
    "    return bce, acc, f1, roc_auc\n",
    "\n",
    "def save_predictions_to_csv(labels, predictions, file_path):\n",
    "    \"\"\"Save the test_labels and predictions to a CSV.\"\"\"\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({\n",
    "        \"True_Labels\": labels,\n",
    "        \"Predictions\": predictions\n",
    "    })\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Prediction results saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = params_vertical_gnn\n",
    "n_repetitions = 1\n",
    "train_data_root_path = './data/graph_data/data_DICT_train/'\n",
    "train_data_raw_filename = 'data_DICT_train.csv'\n",
    "test_data_root_path = './data/graph_data/data_DICT_test/'\n",
    "test_data_raw_filename = 'data_DICT_test.csv'\n",
    "path_to_save_trained_model = './GNN_models/vertical/'\n",
    "\n",
    "val_bce_list = []\n",
    "val_acc_list = []\n",
    "val_f1_list = []\n",
    "val_roc_auc_list = []\n",
    "\n",
    "bce_list = []\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "dataset_for_cv = LoadDICTDataset(root=train_data_root_path, raw_filename=train_data_raw_filename)\n",
    "test_dataset = LoadDICTDataset(root=test_data_root_path, raw_filename=test_data_raw_filename)\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "for repeat in range(n_repetitions):\n",
    "    repeat_val_bce_list = []\n",
    "    repeat_val_acc_list = []\n",
    "    repeat_val_f1_list = []\n",
    "    repeat_val_roc_auc_list = []\n",
    "    \n",
    "    repeat_bce_list = []\n",
    "    repeat_acc_list = []\n",
    "    repeat_f1_list = []\n",
    "    repeat_roc_auc_list = []\n",
    "\n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        seed_everything(SEED_NO)\n",
    "        train_dataset= []\n",
    "        valid_dataset = []\n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(torch.load(f'./data/graph_data/data_DICT_train/processed/molecule_{t_idx}.pt'))\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(torch.load(f'./data/graph_data/data_DICT_train/processed/molecule_{v_idx}.pt'))\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False)\n",
    "\n",
    "        run_training(train_loader, valid_loader, params, os.path.join(path_to_save_trained_model, f'vertical_repeat_{repeat}_fold_{fold_no}.pt'))\n",
    "        val_bce, val_acc, val_f1, val_roc_auc = run_validation(valid_loader, params, os.path.join(path_to_save_trained_model, f'vertical_repeat_{repeat}_fold_{fold_no}.pt'))\n",
    "        bce, acc, f1, roc_auc = run_testing(test_loader, params, os.path.join(path_to_save_trained_model, f'vertical_repeat_{repeat}_fold_{fold_no}.pt'))\n",
    "        \n",
    "        repeat_val_bce_list.append(val_bce)\n",
    "        repeat_val_acc_list.append(val_acc)\n",
    "        repeat_val_f1_list.append(val_f1)\n",
    "        repeat_val_roc_auc_list.append(val_roc_auc)\n",
    "        \n",
    "        repeat_bce_list.append(bce)\n",
    "        repeat_acc_list.append(acc)\n",
    "        repeat_f1_list.append(f1)\n",
    "        repeat_roc_auc_list.append(roc_auc)\n",
    "        \n",
    "        val_bce_list.append(val_bce)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_f1_list.append(val_f1)\n",
    "        val_roc_auc_list.append(val_roc_auc)\n",
    "        \n",
    "        bce_list.append(bce)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "    # Output statistics for validation and CV results for the repeat\n",
    "    print(f'Statistics for repeat {repeat}:')\n",
    "    print(f'Validation - BCE: {np.mean(repeat_val_bce_list):.3f}±{np.std(repeat_val_bce_list):.3f}')\n",
    "    print(f'Validation - ACC: {np.mean(repeat_val_acc_list):.3f}±{np.std(repeat_val_acc_list):.3f}')\n",
    "    print(f'Validation - F1: {np.mean(repeat_val_f1_list):.3f}±{np.std(repeat_val_f1_list):.3f}')\n",
    "    print(f'Validation - ROC_AUC: {np.mean(repeat_val_roc_auc_list):.3f}±{np.std(repeat_val_roc_auc_list):.3f}')\n",
    "\n",
    "    print(f'test - BCE: {np.mean(repeat_bce_list):.3f}±{np.std(repeat_bce_list):.3f}')\n",
    "    print(f'test - ACC: {np.mean(repeat_acc_list):.3f}±{np.std(repeat_acc_list):.3f}')\n",
    "    print(f'test - F1: {np.mean(repeat_f1_list):.3f}±{np.std(repeat_f1_list):.3f}')\n",
    "    print(f'test - ROC_AUC: {np.mean(repeat_roc_auc_list):.3f}±{np.std(repeat_roc_auc_list):.3f}')\n",
    "\n",
    "val_bce_arr = np.array(val_bce_list)\n",
    "val_mean_bce = np.mean(val_bce_arr)\n",
    "val_sd_bce = np.std(val_bce_arr)\n",
    "print(f'validation bce:{val_mean_bce:.3f}±{val_sd_bce:.3f}')\n",
    "\n",
    "val_acc_arr = np.array(val_acc_list)\n",
    "val_acc_mean= np.mean(val_acc_arr)\n",
    "val_acc_sd = np.std(val_acc_arr)\n",
    "print(f'validation acc:{val_acc_mean:.3f}±{val_acc_sd:.3f}')\n",
    "\n",
    "val_f1_arr = np.array(val_f1_list)\n",
    "val_f1_mean= np.mean(val_f1_arr)\n",
    "val_f1_sd = np.std(val_f1_arr)\n",
    "print(f'validation f1: {val_f1_mean:.3f}±{val_f1_sd:.3f}')\n",
    "\n",
    "val_roc_auc_arr = np.array(val_roc_auc_list)\n",
    "val_roc_auc_mean= np.mean(val_roc_auc_arr)\n",
    "val_roc_auc_sd = np.std(val_roc_auc_arr)\n",
    "print(f'validation roc_auc: {val_roc_auc_mean:.3f}±{val_roc_auc_sd:.3f}')\n",
    "\n",
    "bce_arr = np.array(bce_list)\n",
    "mean_bce = np.mean(bce_arr)\n",
    "sd_bce = np.std(bce_arr)\n",
    "print(f'bce:{mean_bce:.3f}±{sd_bce:.3f}')\n",
    "\n",
    "acc_arr = np.array(acc_list)\n",
    "acc_mean= np.mean(acc_arr)\n",
    "acc_sd = np.std(acc_arr)\n",
    "print(f'acc:{acc_mean:.3f}±{acc_sd:.3f}')\n",
    "\n",
    "f1_arr = np.array(f1_list)\n",
    "f1_mean= np.mean(f1_arr)\n",
    "f1_sd = np.std(f1_arr)\n",
    "print(f'f1: {f1_mean:.3f}±{f1_sd:.3f}')\n",
    "\n",
    "roc_auc_arr = np.array(roc_auc_list)\n",
    "roc_auc_mean= np.mean(roc_auc_arr)\n",
    "roc_auc_sd = np.std(roc_auc_arr)\n",
    "print(f'roc_auc: {roc_auc_mean:.3f}±{roc_auc_sd:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
